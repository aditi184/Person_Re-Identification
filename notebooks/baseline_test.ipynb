{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "# from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from model import ClassBlock, LATransformer, LATransformerTest\n",
    "from utils import save_network, update_summary, get_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "# device = \"cpu\" \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 8\n",
    "lr = 3e-4\n",
    "gamma = 0.7\n",
    "lmbd = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda3/envs/cv/lib/python3.7/site-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "transform_query_list = [\n",
    "    transforms.Resize((224,224), interpolation=3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    "transform_gallery_list = [\n",
    "    transforms.Resize(size=(224,224),interpolation=3), #Image.BICUBIC\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    "data_transforms = {\n",
    "    'query': transforms.Compose( transform_query_list ),\n",
    "    'gallery': transforms.Compose(transform_gallery_list),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "image_datasets = {}\n",
    "data_dir = \"/home/shubham/CVP/data/val\"\n",
    "# data_dir = \"/home/shubham/CVP/test\"\n",
    "\n",
    "image_datasets['query'] = datasets.ImageFolder(os.path.join(data_dir, 'query'),\n",
    "                                          data_transforms['query'])\n",
    "image_datasets['gallery'] = datasets.ImageFolder(os.path.join(data_dir, 'gallery'),\n",
    "                                          data_transforms['gallery'])\n",
    "query_loader = DataLoader(dataset = image_datasets['query'], batch_size=batch_size, shuffle=False )\n",
    "gallery_loader = DataLoader(dataset = image_datasets['gallery'], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class_names = image_datasets['query'].classes\n",
    "print(len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LATransformer(nn.Module):\n",
    "    def __init__(self, ViT, lmbd, num_classes=751, test=False):\n",
    "        super(LATransformer, self).__init__()\n",
    "        self.test = test\n",
    "        self.class_num = num_classes # output number of classes\n",
    "        \n",
    "        # ViT model\n",
    "        self.model = ViT\n",
    "        self.model.head.requires_grad_ = False \n",
    "        self.cls_token = self.model.cls_token # 1, 1, 768\n",
    "        self.pos_embed = self.model.pos_embed # 1, 197, 768\n",
    "\n",
    "        # these are ViT model internal hyper-parameters (FIXED) \n",
    "        # self.num_blocks = 12 # number of sequential blocks in ViT\n",
    "        \n",
    "        # there are 196 patches in each image; thus, we split them into 14 x 14 grid\n",
    "        self.num_rows = 14 \n",
    "        self.num_cols = 14\n",
    "\n",
    "        # Locally aware network\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((self.num_rows,768))\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "        if not self.test:\n",
    "            # ensemble of classifiers\n",
    "            for i in range(self.num_rows):\n",
    "                name = 'classifier'+str(i)\n",
    "                setattr(self, name, FC_Classifier(input_dim=768, num_classes=self.class_num, droprate=0.5, num_bottleneck=256, return_features=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape = 32, 3, 224, 224\n",
    "        \n",
    "        # Divide input image into patch embeddings and add position embeddings\n",
    "        x = self.model.patch_embed(x) # 32, 196, 768\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # 32, 1, 768\n",
    "        x = torch.cat((cls_token, x), dim=1) # 32, 197, 768\n",
    "        trnsfrmr_inp = self.model.pos_drop(x + self.pos_embed) # dropout with p = 0; idk!\n",
    "        \n",
    "        # Feed forward the x = (patch_embeddings+position_embeddings) through transformer blocks\n",
    "        # for i in range(self.num_blocks):\n",
    "        # x = self.model.blocks[i](x)\n",
    "        x = self.model.blocks(trnsfrmr_inp)\n",
    "        x_trnsfrmr_encdd = self.model.norm(x) # layer normalization; shape = 32, 197, 768\n",
    "        \n",
    "        # extract the cls token\n",
    "        cls_token_out = x_trnsfrmr_encdd[:, 0].unsqueeze(1)\n",
    "        \n",
    "        # Average pool\n",
    "        Q = x_trnsfrmr_encdd[:, 1:]\n",
    "        L = self.avgpool(Q) # 32, 14, 768\n",
    "        \n",
    "        if self.test:\n",
    "            return L\n",
    "        \n",
    "        # Add global cls token to each local token \n",
    "        for i in range(self.num_rows):\n",
    "            out = torch.mul(L[:, i, :], self.lmbd)\n",
    "            L[:,i,:] = torch.div(torch.add(cls_token_out.squeeze(),out), 1+self.lmbd)\n",
    "        \n",
    "        # Locally aware network\n",
    "        part = {}\n",
    "        predict = {}\n",
    "        for i in range(self.num_rows):\n",
    "            part[i] = L[:,i,:] # 32, 768\n",
    "            name = 'classifier'+str(i)\n",
    "            c = getattr(self, name)\n",
    "            predict[i] = c(part[i]) # 32, 751\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LATransformer(\n",
       "  (model): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (pre_logits): Identity()\n",
       "    (head): Linear(in_features=768, out_features=751, bias=True)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(14, 768))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ViT\n",
    "vit_base = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=751)\n",
    "vit_base = vit_base.to(device)\n",
    "\n",
    "# Create La-Transformer\n",
    "model = LATransformer(vit_base, lmbd=lmbd, num_classes=123, test=True).to(device)\n",
    "\n",
    "# Load LA-Transformer\n",
    "save_path = \"/home/shubham/CVP/model/la-tranformer_best.pth\"\n",
    "model.load_state_dict(torch.load(save_path), strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(model, dataloaders):\n",
    "    features = torch.FloatTensor()\n",
    "    for data in tqdm(dataloaders):\n",
    "        img, label = data\n",
    "        img, label = img.to(device), label.to(device)\n",
    "\n",
    "        output = model(img)\n",
    "        features = torch.cat((features, output.detach().cpu()), 0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d271386e464555b6e10924e3f6ec47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3f1a9ef3e243c6ad6b5c432493535b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract Query Features\n",
    "query_feature = extract_feature(model, query_loader)\n",
    "\n",
    "# Extract Gallery Features\n",
    "gallery_feature = extract_feature(model, gallery_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve labels\n",
    "gallery_path = image_datasets['gallery'].imgs\n",
    "query_path = image_datasets['query'].imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gallery_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(img_path):\n",
    "    camera_id = []\n",
    "    labels = []\n",
    "    for path, label in img_path:\n",
    "        cam_id = int(path.split(\"/\")[-1].split(\"_\")[0])\n",
    "#         filename = os.path.basename(path)\n",
    "#         camera = filename.split('_')[0]\n",
    "        labels.append(int(label))\n",
    "        camera_id.append(cam_id)\n",
    "    return camera_id, labels\n",
    "\n",
    "gallery_cam, gallery_label = get_id(gallery_path)\n",
    "query_cam, query_label = get_id(query_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat Averaged GELTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf9fd821e264145ab4eff55df244d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e95f27aa0b94cfab294144419ec6e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concatenated_query_vectors = []\n",
    "for query in tqdm(query_feature):\n",
    "    fnorm = torch.norm(query, p=2, dim=1, keepdim=True)*np.sqrt(14)\n",
    "    query_norm = query.div(fnorm.expand_as(query))\n",
    "    concatenated_query_vectors.append(query_norm.view((-1))) # 14*768 -> 10752\n",
    "\n",
    "concatenated_gallery_vectors = []\n",
    "for gallery in tqdm(gallery_feature):\n",
    "    fnorm = torch.norm(gallery, p=2, dim=1, keepdim=True) *np.sqrt(14)\n",
    "    gallery_norm = gallery.div(fnorm.expand_as(gallery))\n",
    "    concatenated_gallery_vectors.append(gallery_norm.view((-1))) # 14*768 -> 10752"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Similarity using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(10752))\n",
    "index.add_with_ids(np.array([t.numpy() for t in concatenated_gallery_vectors]),np.array(gallery_label))\n",
    "\n",
    "def search(query: str, k=1):\n",
    "    encoded_query = query.unsqueeze(dim=0).numpy()\n",
    "    top_k = index.search(encoded_query, k)\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank1(label, output):\n",
    "    if label==output[1][0][0]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def rank5(label, output):\n",
    "    if label in output[1][0][:5]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def rank10(label, output):\n",
    "    if label in output[1][0][:10]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def calc_map(label, output):\n",
    "    count = 0\n",
    "    score = 0\n",
    "    good = 0\n",
    "    for out in output[1][0]:\n",
    "        count += 1\n",
    "        if out==label:\n",
    "            good += 1            \n",
    "            score += (good/count)\n",
    "    if good==0:\n",
    "        return 0\n",
    "    return score/good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0, Total: 1, Incorrect: 1\r",
      "Correct: 1, Total: 2, Incorrect: 1\r",
      "Correct: 2, Total: 3, Incorrect: 1\r",
      "Correct: 3, Total: 4, Incorrect: 1\r",
      "Correct: 4, Total: 5, Incorrect: 1\r",
      "Correct: 5, Total: 6, Incorrect: 1\r",
      "Correct: 6, Total: 7, Incorrect: 1\r",
      "Correct: 7, Total: 8, Incorrect: 1\r",
      "Correct: 8, Total: 9, Incorrect: 1\r",
      "Correct: 9, Total: 10, Incorrect: 1\r",
      "Correct: 10, Total: 11, Incorrect: 1\r",
      "Correct: 11, Total: 12, Incorrect: 1\r",
      "Correct: 12, Total: 13, Incorrect: 1\r",
      "Correct: 13, Total: 14, Incorrect: 1\r",
      "Correct: 14, Total: 15, Incorrect: 1\r",
      "Correct: 15, Total: 16, Incorrect: 1\r",
      "Correct: 16, Total: 17, Incorrect: 1\r",
      "Correct: 17, Total: 18, Incorrect: 1\r",
      "Correct: 18, Total: 19, Incorrect: 1\r",
      "Correct: 19, Total: 20, Incorrect: 1\r",
      "Correct: 20, Total: 21, Incorrect: 1\r",
      "Correct: 21, Total: 22, Incorrect: 1\r",
      "Correct: 22, Total: 23, Incorrect: 1\r",
      "Correct: 22, Total: 24, Incorrect: 2\r",
      "Correct: 22, Total: 25, Incorrect: 3\r",
      "Correct: 23, Total: 26, Incorrect: 3\r",
      "Correct: 24, Total: 27, Incorrect: 3\r",
      "Correct: 25, Total: 28, Incorrect: 3\r",
      "Rank1: 0.8928571428571429, Rank5: 0.9285714285714286, Rank10: 1.0, mAP: 0.8852698817622288\n"
     ]
    }
   ],
   "source": [
    "rank1_score = 0\n",
    "rank5_score = 0\n",
    "rank10_score = 0\n",
    "ap = 0\n",
    "count = 0\n",
    "for query, label in zip(concatenated_query_vectors, query_label):\n",
    "    count += 1\n",
    "    label = label\n",
    "    output = search(query, k=10)\n",
    "#     print(output)\n",
    "    rank1_score += rank1(label, output) \n",
    "    rank5_score += rank5(label, output) \n",
    "    rank10_score += rank10(label, output) \n",
    "    print(\"Correct: {}, Total: {}, Incorrect: {}\".format(rank1_score, count, count-rank1_score), end=\"\\r\")\n",
    "    ap += calc_map(label, output)\n",
    "\n",
    "print(\"Rank1: {}, Rank5: {}, Rank10: {}, mAP: {}\".format(rank1_score/len(query_feature), \n",
    "                                                         rank5_score/len(query_feature), \n",
    "                                                         rank10_score/len(query_feature), ap/len(query_feature)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experimental results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. seed:42, epochs:10, Rank1: 0.9285714285714286, Rank5: 0.9285714285714286, Rank10: 0.9642857142857143, mAP: 0.9170920335726883\n",
    "2. seed:42, epochs:15, Rank1: 0.9285714285714286, Rank5: 0.9642857142857143, Rank10: 0.9642857142857143, mAP: 0.9152374822283411\n",
    "3. seed:0, epochs:15, Rank1: 0.8928571428571429, Rank5: 0.9285714285714286, Rank10: 1.0, mAP: 0.8852698817622288\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
