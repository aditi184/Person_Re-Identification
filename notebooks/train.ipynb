{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "import timm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "# device = \"cpu\" \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "lr = 3e-4\n",
    "gamma = 0.7\n",
    "unfreeze_after = 2 # unfreeze transformer blocks after 2 epochs\n",
    "lr_decay = .8\n",
    "lmbd = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train_list = [\n",
    "    transforms.Resize((224,224), interpolation=3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    "transform_val_list = [\n",
    "    transforms.Resize(size=(224,224),interpolation=3), #Image.BICUBIC\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose( transform_train_list ),\n",
    "    'val': transforms.Compose(transform_val_list),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/home/shubham/CVP/TrainData_split/\"\n",
    "# data_dir = \"/home/shubham/CVP/data/\"\n",
    "\n",
    "# image_datasets['train'] = datasets.ImageFolder(os.path.join(data_dir, 'train'),\n",
    "#                                           data_transforms['train'])\n",
    "# image_datasets['val'] = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n",
    "#                                           data_transforms['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dir = \"/home/shubham/CVP/data/train/\"\n",
    "# val_dir = \"/home/shubham/CVP/data/val/all_imgs/\"\n",
    "\n",
    "image_datasets = {}\n",
    "image_datasets['train'] = datasets.ImageFolder(train_dir, data_transforms['train'])\n",
    "# image_datasets['val'] = datasets.ImageFolder(val_dir, data_transforms['val'])\n",
    "\n",
    "train_loader = DataLoader(dataset = image_datasets['train'], batch_size=batch_size, shuffle=True )\n",
    "# valid_loader = DataLoader(dataset = image_datasets['val'], batch_size=batch_size, shuffle=True)\n",
    "class_names = image_datasets['train'].classes # '001','003', etc\n",
    "print(len(class_names))\n",
    "# print(len(image_datasets['val'].classes)) # '001','003', etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ViT\n",
    "vit_base = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=751)\n",
    "vit_base = vit_base.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LA Transformer\n",
    "model = LATransformer(ViT=vit_base, lmbd=lmbd, num_classes=62).to(device) # len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_all_blocks(model):\n",
    "    # frozen_blocks = 12\n",
    "    assert len(model.model.blocks) == 12\n",
    "    for block in model.model.blocks: # [:frozen_blocks]\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad=False\n",
    "\n",
    "def unfreeze_block(model, block_num = 1):\n",
    "    # unfreeze transformer blocks from last\n",
    "    for block in model.model.blocks[11-block_num :]:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad=True\n",
    "    return model\n",
    "\n",
    "def save_network(network, model_dir, name):\n",
    "    save_path = os.path.join(model_dir, name + \".pth\")\n",
    "    torch.save(network.cpu().state_dict(), save_path)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        network.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    epoch_accuracy, epoch_loss = 0, 0\n",
    "    total_samples, correct_predictions = 0, 0\n",
    "    for data, target in tqdm(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # predictions\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        score = 0.0\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        for k, v in output.items():\n",
    "            score += sm(output[k])\n",
    "        _, preds = torch.max(score.data, 1)\n",
    "        \n",
    "        # backpropagation through ensemble\n",
    "        # loss = 0.0\n",
    "        # for k,v in output.items():\n",
    "        #     loss += loss_fn(output[k], target)\n",
    "        loss = 0.0\n",
    "        for loss_function in loss_fn:\n",
    "            for k,v in output.items():\n",
    "                loss += loss_function(output[k], target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print(preds, target.data)\n",
    "        # acc = (preds == target.data).float().mean()\n",
    "        # print(acc)\n",
    "        \n",
    "        # print(acc)\n",
    "        # epoch_loss += loss/len(loader)\n",
    "        # epoch_accuracy += acc / len(loader)\n",
    "        # if acc:\n",
    "        #     print(acc, epreds, target.data)\n",
    "        \n",
    "        epoch_loss += (loss.item()/data.shape[0])\n",
    "        correct_predictions += (preds.eq(target.data).sum().item())\n",
    "        total_samples += data.size(0)\n",
    "        epoch_accuracy = correct_predictions/total_samples\n",
    "        # print(f\"Epoch : {epoch}; loss : {epoch_loss:.4f}; acc: {epoch_accuracy:.4f}\", end=\"\\r\")\n",
    "\n",
    "    # print(\"total_samples\", total_samples, \"correct\", correct_predictions)\n",
    "    epoch_loss /= len(loader)\n",
    "    return OrderedDict([('train_loss', epoch_loss), (\"train_accuracy\", epoch_accuracy)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_one_epoch(epoch, model, loader, loss_fn):\n",
    "#     model.eval()\n",
    "#     epoch_accuracy, epoch_loss = 0, 0\n",
    "#     total_samples, correct_predictions = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in tqdm(loader):\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "\n",
    "#             # predictions\n",
    "#             output = model(data)\n",
    "#             score = 0.0\n",
    "#             sm = nn.Softmax(dim=1)\n",
    "#             for k, v in output.items():\n",
    "#                 score += sm(output[k])\n",
    "#             _, preds = torch.max(score.data, 1)\n",
    "\n",
    "#             # backpropagation through ensemble\n",
    "#             loss = 0.0\n",
    "#             for k,v in output.items():\n",
    "#                 loss += loss_fn(output[k], target)\n",
    "\n",
    "#             epoch_loss += (loss.item()/data.shape[0])\n",
    "#             correct_predictions += (preds.eq(target.data).sum().item())\n",
    "#             total_samples += data.size(0)\n",
    "#             epoch_accuracy = correct_predictions/total_samples\n",
    "#             # print(f\"Epoch : {epoch}; loss : {epoch_loss:.4f}; acc: {epoch_accuracy:.4f}\", end=\"\\r\")\n",
    "\n",
    "#     # print(\"total_samples\", total_samples, \"correct\", correct_predictions)\n",
    "#     epoch_loss /= len(loader)\n",
    "#     return OrderedDict([('val_loss', epoch_loss), (\"val_accuracy\", epoch_accuracy)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"la-tf++_final\"\n",
    "model_dir = \"/home/shubham/CVP/model/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_all_blocks(model)\n",
    "unfreeze_block_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Triplet loss with hard positive/negative mining.\n",
    "    Reference:\n",
    "    Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n",
    "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py.\n",
    "    Args:\n",
    "        margin (float): margin for triplet.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.3, mutual_flag = False):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
    "        self.mutual = mutual_flag\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: feature matrix with shape (batch_size, feat_dim)\n",
    "            targets: ground truth labels with shape (num_classes)\n",
    "        \"\"\"\n",
    "        n = inputs.size(0)\n",
    "        # inputs = 1. * inputs / (torch.norm(inputs, 2, dim=-1, keepdim=True).expand_as(inputs) + 1e-12)\n",
    "        # Compute pairwise distance, replace by the official when merged\n",
    "        dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)\n",
    "        dist = dist + dist.t()\n",
    "        dist.addmm_(1, -2, inputs, inputs.t())\n",
    "        dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
    "        # For each anchor, find the hardest positive and negative\n",
    "        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n",
    "        dist_ap, dist_an = [], []\n",
    "        for i in range(n):\n",
    "            dist_ap.append(dist[i][mask[i]].max().unsqueeze(0))\n",
    "            dist_an.append(dist[i][mask[i] == 0].min().unsqueeze(0))\n",
    "        dist_ap = torch.cat(dist_ap)\n",
    "        dist_an = torch.cat(dist_an)\n",
    "        # Compute ranking hinge loss\n",
    "        y = torch.ones_like(dist_an)\n",
    "        loss = self.ranking_loss(dist_an, dist_ap, y)\n",
    "        if self.mutual:\n",
    "            return loss, dist\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLabelSmooth(nn.Module):\n",
    "    \"\"\"Cross entropy loss with label smoothing regularizer.\n",
    "    Reference:\n",
    "    Szegedy et al. Rethinking the Inception Architecture for Computer Vision. CVPR 2016.\n",
    "    Equation: y = (1 - epsilon) * y + epsilon / K.\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        epsilon (float): weight.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=62, epsilon=0.1, use_gpu=True):\n",
    "        super(CrossEntropyLabelSmooth, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "        self.use_gpu = use_gpu\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)\n",
    "            targets: ground truth labels with shape (num_classes)\n",
    "        \"\"\"\n",
    "        log_probs = self.logsoftmax(inputs)\n",
    "        targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)\n",
    "        if self.use_gpu: targets = targets.cuda()\n",
    "        targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n",
    "        loss = (- targets * log_probs).mean(0).sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = TripletLoss()\n",
    "# criterion = CrossEntropyLabelSmooth()\n",
    "criterion = [CrossEntropyLabelSmooth(), TripletLoss()]\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(),weight_decay=5e-4, lr=lr)\n",
    "\n",
    "# # scheduler\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"training...\")\n",
    "# num_eps = 10\n",
    "# pbar = tqdm(np.arange(num_eps).tolist())\n",
    "for epoch in range(num_epochs):\n",
    "    # if epoch == num_epochs//2:\n",
    "    #    criterion = TripletLoss()\n",
    "\n",
    "    if epoch % unfreeze_after == 0: # and epoch != 0:\n",
    "        unfreeze_block_id += 1\n",
    "        model = unfreeze_block(model, unfreeze_block_id)\n",
    "        optimizer.param_groups[0]['lr'] *= lr_decay \n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        # print(f\"Unfrozen Blocks: {unfreeze_block_id}, Current lr: {optimizer.param_groups[0]['lr']}, Trainable Params: {trainable_params}\")\n",
    "\n",
    "    train_metrics = train_one_epoch(epoch, model, train_loader, optimizer, criterion)\n",
    "    # val_metrics = eval_one_epoch(epoch, model, valid_loader, criterion)\n",
    "    ta = train_metrics['train_accuracy']\n",
    "    tl = train_metrics['train_loss']\n",
    "    # va = val_metrics['val_accuracy']\n",
    "    # vl = val_metrics['val_loss']\n",
    "    # pbar.set_description(f\"Train Acc : {ta}, Train Loss : {tl}, Val Acc : {va}, Val Loss : {vl}\")\n",
    "    \n",
    "    print(f\"Epoch : {epoch}; trainacc : {ta:.4f}\")\n",
    "    # print(f\"Epoch : {epoch}; trainacc : {ta:.4f}; valacc: {va:.4f}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_network(model, model_dir, model_name) \n",
    "print(model_name +\" saved at \" + model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vit_base.head.requires_grad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y = next(iter(train_loader))\n",
    "# print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x.shape)\n",
    "# x = vit_base.patch_embed(x)\n",
    "# print(x.shape)\n",
    "# print()\n",
    "\n",
    "# print(vit_base.cls_token.shape, vit_base.pos_embed.shape)\n",
    "# cls_token = vit_base.cls_token.expand(x.shape[0], -1, -1) \n",
    "# print(cls_token.shape)\n",
    "# x = torch.cat((cls_token, x), dim=1)\n",
    "# print(x.shape)\n",
    "# x = vit_base.pos_drop(x + vit_base.pos_embed)\n",
    "# print(x.shape)\n",
    "# print()\n",
    "\n",
    "# # Feed forward the x = (patch_embeddings+position_embeddings) through transformer blocks\n",
    "# # for i in range(12):\n",
    "# x = vit_base.blocks(x)\n",
    "# x = vit_base.norm(x) # layer normalization\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the cls token\n",
    "# cls_token_out = x[:, 0].unsqueeze(1)\n",
    "# print(cls_token_out.shape)\n",
    "\n",
    "# # Average pool\n",
    "# avgpool = nn.AdaptiveAvgPool2d(output_size = (14, 768))\n",
    "# print(x.shape)\n",
    "# x = avgpool(x[:, 1:]) # input is 32,196,768\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.tensor([1.0,2])\n",
    "# print(t)\n",
    "# t.requires_grad_ = True\n",
    "# print(t)\n",
    "# t.requires_grad = True\n",
    "# print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
